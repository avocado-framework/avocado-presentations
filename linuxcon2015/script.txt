* Developer working on (shiny new) "bind" mount (make a part of the fs available on another location):
  - This developer is working on either userspace (mount command itself) or kernel level
  - Every time a change is made, we want to recompile the software and re-run the test (if hacking kernel, include a reboot).
  - The developer configures system (mount-test-setup.sh):

    #!/bin/bash
    # (must be run as root)
    mkdir -p /srv/tmp
    grep -q '/tmp /srv/tmp none bind,noauto,user 0 0' /etc/fstab || echo '/tmp /srv/tmp none bind,noauto,user 0 0' >> /etc/fstab


  - Writes a fair test:

    #!/bin/bash -e

    # Make sure the mount point is not mounted previously
    grep -q -v /srv/tmp /proc/mounts || umount /srv/tmp

    mount /srv/tmp
    grep -q /srv/tmp /proc/mounts && echo 'PASS: Mount test' || echo 'FAIL: Mount test'

    umount /srv/tmp
    grep -q -v /srv/tmp /proc/mounts && echo 'PASS: Umount test' || echo 'FAIL: Umount test'


* Without Avocado
  - $ ./test-mount.sh
  - Looks at the outcome
  - That is not bad. Can we make it better?

* With Avocado
  - $ avocado run test-mount.sh
  - Looks at the outcome
  - $ avocado run test-mount.sh --open-browser
  - Now that's better!

* What did you get?
  - Results persisted (as files)
  - System information collected
  - Better visual output of the outcome
  - Communicate the results to other team members, or to machines

* Sysinfo
  - Was /etc/fstab correctly configured?
  - Let's collect it!

  # echo "/etc/fstab" >> /etc/avocado/sysinfo/files

  - Rerun the test:
  
  $ avocado run test-mount.sh --open-browser

* Is that all?
  - Absolutely not! This is just the some of the "tool" part
  - Remeber: "Avocado is a set of tools ***and libraries*** to help with automated testing.
  - At this point, those libraries are mostly for Python
  - Some support for shell scripts, and more planned

* A moment of thought about test ERRORs and FAILures
  - When writing a non-assertive test, like test-mount.sh, some test
    ERRORs may end up showing as test FAILures
  - It's possible to either eliminate or greatly reduce that
  - Avocado has very well defined test status: PASS, FAIL, ERROR, 

* Let's have a shot at this Python version then
 - turn (SIMPLE) test-mount.sh into (INSTRUMENTED) test-mount.py

from avocado import Test, fail_on
from avocado.utils import process

class BindMount(Test):

    @fail_on
    def test_mount(self):
        # Please don't actually write your test like this
        process.run("grep -q -v /srv/tmp /proc/mounts || umount /srv/tmp",
                    shell=True)
        process.run("mount /srv/tmp")
        process.run("grep /srv/tmp /proc/mounts")

    @fail_on
    def test_umount(self):
        # Please don't actually write your test like this
        process.run("grep -q /srv/tmp /proc/mounts || mount /srv/tmp",
                    shell=True)
        process.run("umount /srv/tmp")
        process.run("grep -v /srv/tmp /proc/mounts")

* Avocado knows they're different

   $ avocado list test-mount.{sh,py}
   SIMPLE       test-mount.sh                
   INSTRUMENTED test-mount.py:BindMount.test

  - Test Code is officially Python, but very shell-like
  - Still, it's an INSTRUMENTED Avocado test

* What do we gain?
  - Dedicated log streams

  ...
  def test(self):
     self.log.info("About to mount")
     process.run("mount /srv/tmp")
  ...

* Is that all we gain?
  - Well, all commands are logged

   ..
   12:04:54 process    L0272 INFO | Running 'mount /srv/tmp'
   12:04:54 process    L0272 INFO | Running 'grep /srv/tmp /proc/mounts'
   12:04:54 process    L0342 DEBUG| [stdout] tmpfs /srv/tmp tmpfs rw,seclabel 0 0
   ...

* I'm still not convinced, do I get anything else?
  - Avocado has about XX unique features
  - Instrumentation with GDB
  - Instrumentation with any wrapper you write (some included)
  - Running the same test on a remote system transparently
  - Running the same test on a VM, including snapshotting and
    reverting its state
  - With a single test, you can cover a large test matrix (test combinations)
    (that's the Multiplexer feature, we'll get back on that)
  - Your own feature, by means of new plugins

* Suppose you're still interested
  - Let's get back to the test and make it better:
  * Make it more Pythonic
  * Make it more assertful

* Shall I present "test-mount-v2.py":

    from avocado import Test, fail_on
    from avocado.utils import process

    class BindMount(Test):

        @fail_on(process.CmdError)
        def test_mount(self):
            if "/srv/tmp" in open("/proc/mounts").read():
                process.run("umount /srv/tmp")

            process.run("mount /srv/tmp")
            self.assertIn("/srv/tmp", open("/proc/mounts").read())

        @fail_on(process.CmdError)
        def test_umount(self):
            if not "/srv/tmp" in open("/proc/mounts").read():
                process.run("mount /srv/tmp")

            process.run("umount /srv/tmp")
            self.assertNotIn("/srv/tmp", open("/proc/mounts").read())


* What if we could try multiple mount points within the same test?

  - New mount point and fstab entry with "ro" option

    #!/bin/bash
    # (must be run as root)
    mkdir -p /srv/tmp
    grep -q '/tmp /srv/tmp none bind,noauto,user 0 0' /etc/fstab || echo '/tmp /srv/tmp none bind,noauto,user 0 0' >> /etc/fstab

    mkdir -p /srv/ro
    grep -q '/tmp /srv/ro none bind,noauto,user,ro 0 0' /etc/fstab || echo '/tmp /srv/ro none bind,noauto,user,ro 0 0' >> /etc/fstab

  - Let's use parameters instead of fixed values on the test
  - Parameters will come from a Multiplex file:

   !mux			 
   srv_tmp:		 
       mountpoint: /srv/tmp
   srv_ro:		 
       mountpoint: /srv/ro

* The parameterized test itself:

from avocado import Test, fail_on
from avocado.utils import process

class BindMount(Test):

    DEFAULT_MOUNT_POINT = "/srv/tmp"

    @fail_on(process.CmdError)
    def test_mount(self):
        mount_point = self.params.get("mountpoint", default=self.DEFAULT_MOUNT_POINT)
        if mount_point in open("/proc/mounts").read():
            process.run("umount %s" % mount_point)

        process.run("mount %s" % mount_point)
        self.assertIn(mount_point, open("/proc/mounts").read())

    @fail_on(process.CmdError)
    def test_umount(self):
        mount_point = self.params.get("mountpoint", default=self.DEFAULT_MOUNT_POINT)
        if not mount_point in open("/proc/mounts").read():
            process.run("mount %s" % mount_point)

        process.run("umount %s" % mount_point)
        self.assertNotIn(mount_point, open("/proc/mounts").read())

* Setting up a remote machine

 - Create an SSH key for the best experience

  $ ssh-keygen
  $ ssh-copy-id -i ~/.ssh/id_rsa user@host

 - On the remote machine:

  # yum install avocado
  # mount-setup-v2.sh

* Running the same test on a remote machine

  $ avocado run test-mount-v3.py --remote-username=user --remote-machine=host

* Sharing results
 - Setup an HTTP server (as root)

 # apache-setup.sh

 - Setup avocado job results directory

 $ apache-public-html.sh

- You could bookmark:

  http://my-host/~user/job-results/

  http://colleague-host/~user/job-results/latest/html/results.html
